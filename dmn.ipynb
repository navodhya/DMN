{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DMN FINAL",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/navodhya/DMN/blob/master/dmn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "oaL3GiMMUnlh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import urllib\n",
        "import sys\n",
        "import os\n",
        "import zipfile\n",
        "import tarfile\n",
        "import json \n",
        "import hashlib\n",
        "import re\n",
        "import itertools\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Na6Ou1hMWu3w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ggWtVk9lW1R7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_set_post_file = '/content/drive/My Drive/GloVe/en-10k/qa11_basic-coreference_train.txt'\n",
        "test_set_post_file = '/content/drive/My Drive/GloVe/en-10k/qa11_basic-coreference_test.txt'\n",
        "glove_vectors_file = '/content/drive/My Drive/GloVe/glove.6B.50d.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mvnIf863VGHM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "glove_wordmap = {}\n",
        "with open(glove_vectors_file, \"r\", encoding=\"utf8\") as glove:\n",
        "    for line in glove:\n",
        "        name, vector = tuple(line.split(\" \", 1))\n",
        "        glove_wordmap[name] = np.fromstring(vector, sep=\" \")\n",
        "        \n",
        "        wvecs = []\n",
        "for item in glove_wordmap.items():\n",
        "    wvecs.append(item[1])\n",
        "s = np.vstack(wvecs)\n",
        "\n",
        "v = np.var(s,0) \n",
        "m = np.mean(s,0) \n",
        "RS = np.random.RandomState()\n",
        "\n",
        "def fill_unk(unk):\n",
        "    global glove_wordmap\n",
        "    glove_wordmap[unk] = RS.multivariate_normal(m,np.diag(v))\n",
        "    return glove_wordmap[unk]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wTjjuBT5VVLx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sentence2sequence(sentence):\n",
        "\n",
        "    tokens = sentence.strip('\"(),-').lower().split(\" \")\n",
        "    rows = []\n",
        "    words = []\n",
        "\n",
        "    for token in tokens:\n",
        "        i = len(token)\n",
        "        while len(token) > 0:\n",
        "            word = token[:i]\n",
        "            if word in glove_wordmap:\n",
        "                rows.append(glove_wordmap[word])\n",
        "                words.append(word)\n",
        "                token = token[i:]\n",
        "                i = len(token)\n",
        "                continue\n",
        "            else:\n",
        "                i = i-1\n",
        "            if i == 0:\n",
        "\n",
        "                rows.append(fill_unk(token))\n",
        "                words.append(token)\n",
        "                break\n",
        "    return np.array(rows), words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HlHLMl0iVaxu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def contextualize(set_file):\n",
        "\n",
        "    data = []\n",
        "    context = []\n",
        "    with open(set_file, \"r\", encoding=\"utf8\") as train:\n",
        "        for line in train:\n",
        "            l, ine = tuple(line.split(\" \", 1))\n",
        "\n",
        "            if l is \"1\":\n",
        "\n",
        "                context = []\n",
        "            if \"\\t\" in ine: \n",
        "\n",
        "                question, answer, support = tuple(ine.split(\"\\t\"))\n",
        "                data.append((tuple(zip(*context))+\n",
        "                             sentence2sequence(question)+\n",
        "                             sentence2sequence(answer)+\n",
        "                             ([int(s) for s in support.split()],)))\n",
        "            else:\n",
        "                context.append(sentence2sequence(ine[:-1]))\n",
        "    return data\n",
        "train_data = contextualize(train_set_post_file)\n",
        "test_data = contextualize(test_set_post_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hYBZWUtZVdf6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "final_train_data = []\n",
        "def finalize(data):\n",
        "\n",
        "    final_data = []\n",
        "    for cqas in data:\n",
        "        contextvs, contextws, qvs, qws, avs, aws, spt = cqas\n",
        "\n",
        "        lengths = itertools.accumulate(len(cvec) for cvec in contextvs)\n",
        "        context_vec = np.concatenate(contextvs)\n",
        "        context_words = sum(contextws,[])\n",
        "        \n",
        "        sentence_ends = np.array(list(lengths)) \n",
        "        final_data.append((context_vec, sentence_ends, qvs, spt, context_words, cqas, avs, aws))\n",
        "    return np.array(final_data)\n",
        "final_train_data = finalize(train_data)   \n",
        "final_test_data = finalize(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G9CAc9RAVgTB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ELW8tKYRVhKQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "\n",
        "recurrent_cell_size = 256\n",
        "D = 50 \n",
        "learning_rate = 0.005\n",
        "input_p, output_p = 0.9, 0.5\n",
        "batch_size = 128\n",
        "passes = 4\n",
        "weight_decay = 0.00000001\n",
        "training_iterations_count = 400000\n",
        "display_step = 79"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XMns53gtVkaj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Input Module\n",
        "\n",
        "context = tf.placeholder(tf.float32, [None, None, D], \"context\")  \n",
        "context_placeholder = context\n",
        "input_sentence_endings = tf.placeholder(tf.int32, [None, None, 2], \"sentence\")\n",
        "input_gru = tf.contrib.rnn.GRUCell(recurrent_cell_size)\n",
        "gru_drop = tf.contrib.rnn.DropoutWrapper(input_gru, input_p, output_p)\n",
        "input_module_outputs, _ = tf.nn.dynamic_rnn(gru_drop, context, dtype=tf.float32, scope = \"input_module\")\n",
        "cs = tf.gather_nd(input_module_outputs, input_sentence_endings)\n",
        "s = input_module_outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ltbfsH4RVnVj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Question Module\n",
        "\n",
        "query = tf.placeholder(tf.float32, [None, None, D], \"query\")\n",
        "input_query_lengths = tf.placeholder(tf.int32, [None, 2], \"query_lengths\")\n",
        "question_module_outputs, _ = tf.nn.dynamic_rnn(gru_drop, query, dtype=tf.float32, scope = tf.VariableScope(True, \"input_module\"))\n",
        "q = tf.gather_nd(question_module_outputs, input_query_lengths)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AglQKUEUVp18",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Episodic Memory\n",
        "\n",
        "size = tf.stack([tf.constant(1),tf.shape(cs)[1], tf.constant(1)])\n",
        "re_q = tf.tile(tf.reshape(q,[-1,1,recurrent_cell_size]),size)\n",
        "\n",
        "output_size = 1 \n",
        "\n",
        "attend_init = tf.random_normal_initializer(stddev=0.1)\n",
        "w_1 = tf.get_variable(\"attend_w1\", [1,recurrent_cell_size*7, recurrent_cell_size], \n",
        "                      tf.float32, initializer = attend_init)\n",
        "w_2 = tf.get_variable(\"attend_w2\", [1,recurrent_cell_size, output_size], \n",
        "                      tf.float32, initializer = attend_init)\n",
        "\n",
        "b_1 = tf.get_variable(\"attend_b1\", [1, recurrent_cell_size], tf.float32, initializer = attend_init)\n",
        "b_2 = tf.get_variable(\"attend_b2\", [1, output_size], tf.float32, initializer = attend_init)\n",
        "\n",
        "tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, tf.nn.l2_loss(w_1))\n",
        "tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, tf.nn.l2_loss(b_1))\n",
        "tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, tf.nn.l2_loss(w_2))\n",
        "tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, tf.nn.l2_loss(b_2))\n",
        "\n",
        "def attention(c, mem, existing_facts):\n",
        "\n",
        "    with tf.variable_scope(\"attending\") as scope:\n",
        "\n",
        "        attending = tf.concat([c, mem, re_q, c * re_q,  c * mem, (c-re_q)**2, (c-mem)**2], 2)\n",
        "\n",
        "        m1 = tf.matmul(attending * existing_facts, \n",
        "                       tf.tile(w_1, tf.stack([tf.shape(attending)[0],1,1]))) * existing_facts\n",
        "\n",
        "        bias_1 = b_1 * existing_facts\n",
        "        \n",
        "        tnhan = tf.nn.relu(m1 + bias_1)\n",
        "        \n",
        "        m2 = tf.matmul(tnhan, tf.tile(w_2, tf.stack([tf.shape(attending)[0],1,1])))\n",
        "\n",
        "        bias_2 = b_2 * existing_facts\n",
        "        \n",
        "        norm_m2 = tf.nn.l2_normalize(m2 + bias_2, -1)\n",
        "\n",
        "        softmax_idx = tf.where(tf.not_equal(norm_m2, 0))[:,:-1]\n",
        "        softmax_gather = tf.gather_nd(norm_m2[...,0], softmax_idx)\n",
        "        softmax_shape = tf.shape(norm_m2, out_type=tf.int64)[:-1]\n",
        "        softmaxable = tf.SparseTensor(softmax_idx, softmax_gather, softmax_shape)\n",
        "        return tf.expand_dims(tf.sparse_tensor_to_dense(tf.sparse_softmax(softmaxable)),-1)\n",
        "\n",
        "facts_0s = tf.cast(tf.count_nonzero(input_sentence_endings[:,:,-1:],-1,keep_dims=True),tf.float32)\n",
        "\n",
        "\n",
        "with tf.variable_scope(\"Episodes\") as scope:\n",
        "    attention_gru = tf.contrib.rnn.GRUCell(recurrent_cell_size)\n",
        "    \n",
        "    memory = [q]\n",
        "\n",
        "    attends = []\n",
        "    for a in range(passes):\n",
        "\n",
        "        attend_to = attention(cs, tf.tile(tf.reshape(memory[-1],[-1,1,recurrent_cell_size]),size),\n",
        "                              facts_0s)\n",
        "\n",
        "        retain = 1-attend_to\n",
        "\n",
        "        while_valid_index = (lambda state, index: index < tf.shape(cs)[1])\n",
        "        update_state = (lambda state, index: (attend_to[:,index,:] * \n",
        "                                                 attention_gru(cs[:,index,:], state)[0] + \n",
        "                                                 retain[:,index,:] * state))\n",
        "\n",
        "        memory.append(tuple(tf.while_loop(while_valid_index,\n",
        "                          (lambda state, index: (update_state(state,index),index+1)),\n",
        "                           loop_vars = [memory[-1], 0]))[0]) \n",
        "        \n",
        "        attends.append(attend_to)\n",
        "        \n",
        "        scope.reuse_variables()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qbHwOuopV3Fq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Answer Module\n",
        "\n",
        "a0 = tf.concat([memory[-1], q], -1)\n",
        "\n",
        "fc_init = tf.random_normal_initializer(stddev=0.1) \n",
        "\n",
        "with tf.variable_scope(\"answer\"):\n",
        "\n",
        "    w_answer = tf.get_variable(\"weight\", [recurrent_cell_size*2, D], tf.float32, initializer = fc_init)\n",
        "\n",
        "    tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, tf.nn.l2_loss(w_answer)) \n",
        "    \n",
        "    logit = tf.expand_dims(tf.matmul(a0, w_answer),1)\n",
        "    \n",
        "    with tf.variable_scope(\"ending\"):\n",
        "        all_ends = tf.reshape(input_sentence_endings, [-1,2])\n",
        "        range_ends = tf.range(tf.shape(all_ends)[0])\n",
        "        ends_indices = tf.stack([all_ends[:,0],range_ends], axis=1)\n",
        "        ind = tf.reduce_max(tf.scatter_nd(ends_indices, all_ends[:,1],\n",
        "                                          [tf.shape(q)[0], tf.shape(all_ends)[0]]),\n",
        "                            axis=-1)\n",
        "        range_ind = tf.range(tf.shape(ind)[0])\n",
        "        mask_ends = tf.cast(tf.scatter_nd(tf.stack([ind, range_ind], axis=1), \n",
        "                                          tf.ones_like(range_ind), [tf.reduce_max(ind)+1, \n",
        "                                                                    tf.shape(ind)[0]]), bool)\n",
        "\n",
        "        mask = tf.scan(tf.logical_xor,mask_ends, tf.ones_like(range_ind, dtype=bool))\n",
        "        \n",
        "    logits = -tf.reduce_sum(tf.square(context*tf.transpose(tf.expand_dims(\n",
        "                    tf.cast(mask, tf.float32),-1),[1,0,2]) - logit), axis=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IzQC853_WEzX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gold_standard = tf.placeholder(tf.float32, [None, 1, D], \"answer\")\n",
        "with tf.variable_scope('accuracy'):\n",
        "    eq = tf.equal(context, gold_standard)\n",
        "    corrbool = tf.reduce_all(eq,-1)\n",
        "    logloc = tf.reduce_max(logits, -1, keep_dims = True)\n",
        "\n",
        "    locs = tf.equal(logits, logloc)\n",
        "    \n",
        "    correctsbool = tf.reduce_any(tf.logical_and(locs, corrbool), -1)\n",
        "\n",
        "    corrects = tf.where(correctsbool, tf.ones_like(correctsbool, dtype=tf.float32), \n",
        "                        tf.zeros_like(correctsbool,dtype=tf.float32))\n",
        "    \n",
        "    corr = tf.where(corrbool, tf.ones_like(corrbool, dtype=tf.float32), \n",
        "                        tf.zeros_like(corrbool,dtype=tf.float32))\n",
        "with tf.variable_scope(\"loss\"):\n",
        "\n",
        "    loss = tf.nn.sigmoid_cross_entropy_with_logits(logits = tf.nn.l2_normalize(logits,-1),labels = corr)\n",
        "    \n",
        "    total_loss = tf.reduce_mean(loss) + weight_decay * tf.add_n(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "\n",
        "opt_op = optimizer.minimize(total_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Qyc0EE9KWHma",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "init = tf.global_variables_initializer()\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(init)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FTMYOcIaWJ_I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def prep_batch(batch_data, more_data = False):\n",
        "\n",
        "    context_vec, sentence_ends, questionvs, spt, context_words, cqas, answervs, _ = zip(*batch_data)\n",
        "    ends = list(sentence_ends)\n",
        "    maxend = max(map(len, ends))\n",
        "    aends = np.zeros((len(ends), maxend))\n",
        "    for index, i in enumerate(ends):\n",
        "        for indexj, x in enumerate(i):\n",
        "            aends[index, indexj] = x-1\n",
        "    new_ends = np.zeros(aends.shape+(2,))\n",
        "\n",
        "    for index, x in np.ndenumerate(aends):\n",
        "        new_ends[index+(0,)] = index[0]\n",
        "        new_ends[index+(1,)] = x\n",
        "\n",
        "    contexts = list(context_vec)\n",
        "    max_context_length = max([len(x) for x in contexts])\n",
        "    contextsize = list(np.array(contexts[0]).shape)\n",
        "    contextsize[0] = max_context_length\n",
        "    final_contexts = np.zeros([len(contexts)]+contextsize)\n",
        "\n",
        "    contexts = [np.array(x) for x in contexts]\n",
        "    for i, context in enumerate(contexts):\n",
        "        final_contexts[i,0:len(context),:] = context\n",
        "    max_query_length = max(len(x) for x in questionvs)\n",
        "    querysize = list(np.array(questionvs[0]).shape)\n",
        "    querysize[:1] = [len(questionvs),max_query_length]\n",
        "    queries = np.zeros(querysize)\n",
        "    querylengths = np.array(list(zip(range(len(questionvs)),[len(q)-1 for q in questionvs])))\n",
        "    questions = [np.array(q) for q in questionvs]\n",
        "    for i, question in enumerate(questions):\n",
        "        queries[i,0:len(question),:] = question\n",
        "    data = {context_placeholder: final_contexts, input_sentence_endings: new_ends, \n",
        "                            query:queries, input_query_lengths:querylengths, gold_standard: answervs}\n",
        "    return (data, context_words, cqas) if more_data else data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hocHbzPHWMv4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch = np.random.randint(final_test_data.shape[0], size=batch_size*10)\n",
        "batch_data = final_test_data[batch]\n",
        "\n",
        "validation_set, val_context_words, val_cqas = prep_batch(batch_data, True)\n",
        "\n",
        "def train(iterations, batch_size):\n",
        "    training_iterations = range(0, iterations, batch_size)\n",
        "    training_iterations = tqdm(training_iterations)\n",
        "    \n",
        "    accuracy_list = []\n",
        "    loss_list = []\n",
        "    epoc_list = []\n",
        "    k=1\n",
        "    wordz = []\n",
        "    for j in training_iterations:\n",
        "        \n",
        "        batch = np.random.randint(final_train_data.shape[0], size=batch_size)\n",
        "        batch_data = final_train_data[batch]\n",
        "\n",
        "        sess.run([opt_op], feed_dict=prep_batch(batch_data))\n",
        "        if (j/batch_size) % display_step == 0:\n",
        "\n",
        "            # Calculate batch accuracy\n",
        "            acc, ccs, tmp_loss, log, con, cor, loc  = sess.run([corrects, cs, total_loss, logit, context_placeholder,corr, locs], feed_dict=validation_set)\n",
        "\n",
        "            print(\"Iter \" + str(j/batch_size) + \", Minibatch Loss= \", tmp_loss, \"Accuracy= \", np.mean(acc))\n",
        "            accuracy_list.append(np.mean(acc))\n",
        "            loss_list.append(tmp_loss)\n",
        "            epoc_list.append(k)\n",
        "            k=k+1\n",
        "            \n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.style.use('seaborn-white')\n",
        "    plt.plot(epoc_list,accuracy_list, color='steelblue')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.savefig('accuracy.png', dpi=300)\n",
        "    plt.show()\n",
        "    \n",
        "    plt.plot(epoc_list,loss_list, color='steelblue')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.savefig('loss.png', dpi=300)\n",
        "    plt.show() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7h-i4PUvZZ96",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train(training_iterations_count, batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ei59w9JtWYXh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Test Accuracy : \")\n",
        "print(np.mean(sess.run([corrects], feed_dict= prep_batch(final_test_data))[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MxbCOxRCg8G-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ancr = sess.run([corrbool,locs, total_loss, logits, facts_0s, w_1] + attends + [query, cs, question_module_outputs],feed_dict=validation_set)\n",
        "\n",
        "a = ancr[0]\n",
        "n = ancr[1]\n",
        "cr = ancr[2]\n",
        "attenders = np.array(ancr[6:-3]) \n",
        "faq = np.sum(ancr[4], axis=(-1,-2))\n",
        "\n",
        "limit = 5\n",
        "for question in range(min(limit, batch_size)):\n",
        "    plt.yticks(range(passes,0,-1))\n",
        "    plt.ylabel(\"Episode\")\n",
        "    plt.xlabel(\"Question \"+str(question+1))\n",
        "    pltdata = attenders[:,question,:int(faq[question]),0] \n",
        "    pltdata = (pltdata - pltdata.mean()) / ((pltdata.max() - pltdata.min() + 0.001)) * 256\n",
        "    plt.pcolor(pltdata, cmap=plt.cm.Blues, alpha=0.7)\n",
        "    plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "II3al3NFg81H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "indices = np.argmax(n,axis=1)\n",
        "indicesc = np.argmax(a,axis=1)\n",
        "\n",
        "for i,e,cw, cqa in list(zip(indices, indicesc, val_context_words, val_cqas))[:limit]:\n",
        "    ccc = \" \".join(cw)\n",
        "    print(\"TEXT: \",ccc)\n",
        "    print (\"QUESTION: \", \" \".join(cqa[3]))\n",
        "    print (\"RESPONSE: \", cw[i], [\"Correct\", \"Incorrect\"][i!=e])\n",
        "    print(\"EXPECTED: \", cw[e])\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZLVcAK-67vnr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "saver = tf.train.Saver()\n",
        "save_path = saver.save(sess, '/temp/model.ckpt')\n",
        "print(\"Model saved in path: %s\" % save_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7NJlB7vQ8CCv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/temp/model.ckpt.index')\n",
        "files.download('/model.ckpt.data-00000-of-00001')\n",
        "files.download('/temp/model.ckpt.meta')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oICD5ws8PyZq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}